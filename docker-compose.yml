services:
  api:
    build:
      context: .
      dockerfile: docker/Dockerfile.api
    env_file: .env
    environment:
      - LLM_PROVIDER=${LLM_PROVIDER}
      - OLLAMA_HOST=${OLLAMA_HOST}
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - EMBEDDING_BACKEND=${EMBEDDING_BACKEND}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
      - PYTHONIOENCODING=utf-8
      - LC_ALL=C.UTF-8
      - LANG=C.UTF-8
    depends_on:
      - ollama
    ports:
      - "8000:8000"
    volumes:
      - ./:/app
      - hf_cache:/root/.cache/huggingface
      - ./data/rag_store:/app/app/rag/store
    command: ["/bin/bash", "/app/docker/start.sh"]
    mem_limit: 2g
    cpus: 2

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:11434/api/version"]
      interval: 30s
      timeout: 5s
      retries: 10
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_PARALLEL=1
      - OLLAMA_MAX_LOADED_MODELS=1
      - OLLAMA_NOHISTORY=true
      - OLLAMA_KEEP_ALIVE=5m  # держим тёплой 5 минут
      - OLLAMA_LOAD_MMAP=1 
    mem_limit: 16g
    cpus: 6
   

volumes:
  ollama:
  hf_cache:
